{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.007392996108949416\n"
     ]
    }
   ],
   "source": [
    "from difflib import SequenceMatcher\n",
    "with open('file1.txt') as file_1,open('file2.txt') as file_2:\n",
    "    file1_data = file_1.read()\n",
    "    file2_data = file_2.read()\n",
    "    similarity_ratio = SequenceMatcher(None,file1_data,file2_data).ratio()\n",
    "    print(similarity_ratio)  #plagiarism detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.45573272]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect=CountVectorizer(binary=True)\n",
    "c=open('corpus3.txt','r')\n",
    "corpus=[c.read()]\n",
    "vect.fit(corpus)\n",
    "f1=open('file1.txt','r')\n",
    "raw1=f1.read()\n",
    "raw1=raw1.lower()\n",
    "f1.close()\n",
    "f2=open('file2.txt','r')\n",
    "raw2=f2.read()\n",
    "raw2=raw2.lower()\n",
    "f2.close()\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similarity=cosine_similarity(vect.transform([raw1]).toarray(),vect.transform([raw2]))\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.20305918]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect=CountVectorizer(binary=True)\n",
    "c=open('corpus3.txt','r')\n",
    "corpus=[c.read()]\n",
    "vect.fit(corpus)\n",
    "f1=open('file1.txt','r')\n",
    "raw1=f1.read()\n",
    "raw1=raw1.lower()\n",
    "f1.close()\n",
    "f2=open('file2.txt','r')\n",
    "raw2=f2.read()\n",
    "raw2=raw2.lower()\n",
    "f2.close()\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similarity=cosine_similarity(vect.transform([raw1]).toarray(),vect.transform([raw2]))\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.20305918]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect=CountVectorizer(binary=True)\n",
    "c=open('corpus3.txt','r')\n",
    "corpus=[c.read()]\n",
    "vect.fit(corpus)\n",
    "f1=open('file1.txt','r')\n",
    "raw1=f1.read()\n",
    "raw1=raw1.lower()\n",
    "f1.close()\n",
    "f2=open('file2.txt','r')\n",
    "raw2=f2.read()\n",
    "raw2=raw2.lower()\n",
    "f2.close()\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similarity=cosine_similarity(vect.transform([raw1]).toarray(),vect.transform([raw2]))\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.17658579]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect=CountVectorizer(binary=True)\n",
    "c=open('corpus3.txt','r')\n",
    "corpus=[c.read()]\n",
    "vect.fit(corpus)\n",
    "f1=open('file3.txt','r')\n",
    "raw1=f1.read()\n",
    "raw1=raw1.lower()\n",
    "f1.close()\n",
    "f2=open('file2.txt','r')\n",
    "raw2=f2.read()\n",
    "raw2=raw2.lower()\n",
    "f2.close()\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similarity=cosine_similarity(vect.transform([raw1]).toarray(),vect.transform([raw2]))\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.218418]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect=CountVectorizer(binary=True)\n",
    "c=open('corpus3.txt','r')\n",
    "corpus=[c.read()]\n",
    "vect.fit(corpus)\n",
    "f1=open('file1.txt','r')\n",
    "raw1=f1.read()\n",
    "raw1=raw1.lower()\n",
    "f1.close()\n",
    "f2=open('file3.txt','r')\n",
    "raw2=f2.read()\n",
    "raw2=raw2.lower()\n",
    "f2.close()\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similarity=cosine_similarity(vect.transform([raw1]).toarray(),vect.transform([raw2]))\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.39008569]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect=CountVectorizer(binary=True)\n",
    "c=open('corpus3.txt','r')\n",
    "corpus=[c.read()]\n",
    "vect.fit(corpus)\n",
    "f1=open('file1.txt','r')\n",
    "raw1=f1.read()\n",
    "raw1=raw1.lower()\n",
    "f1.close()\n",
    "f2=open('randomfile.txt','r')\n",
    "raw2=f2.read()\n",
    "raw2=raw2.lower()\n",
    "f2.close()\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similarity=cosine_similarity(vect.transform([raw1]).toarray(),vect.transform([raw2]))\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.2042393]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect=CountVectorizer(binary=True)\n",
    "c=open('mixedcorpus.txt','r')\n",
    "corpus=[c.read()]\n",
    "vect.fit(corpus)\n",
    "f1=open('file1.txt','r')\n",
    "raw1=f1.read()\n",
    "raw1=raw1.lower()\n",
    "f1.close()\n",
    "f2=open('randomfile.txt','r')\n",
    "raw2=f2.read()\n",
    "raw2=raw2.lower()\n",
    "f2.close()\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similarity=cosine_similarity(vect.transform([raw1]).toarray(),vect.transform([raw2]))\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.32930651]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect=CountVectorizer(binary=True)\n",
    "c=open('mixedcorpus.txt','r')\n",
    "corpus=[c.read()]\n",
    "vect.fit(corpus)\n",
    "f1=open('file1.txt','r')\n",
    "raw1=f1.read()\n",
    "raw1=raw1.lower()\n",
    "f1.close()\n",
    "f2=open('randomfile.txt','r')\n",
    "raw2=f2.read()\n",
    "raw2=raw2.lower()\n",
    "f2.close()\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similarity=cosine_similarity(vect.transform([raw1]).toarray(),vect.transform([raw2]))\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.18493562]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect=CountVectorizer(binary=True)\n",
    "c=open('mixedcorpus.txt','r')\n",
    "corpus=[c.read()]\n",
    "vect.fit(corpus)\n",
    "f1=open('file1.txt','r')\n",
    "raw1=f1.read()\n",
    "raw1=raw1.lower()\n",
    "f1.close()\n",
    "f2=open('randomfile.txt','r')\n",
    "raw2=f2.read()\n",
    "raw2=raw2.lower()\n",
    "f2.close()\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similarity=cosine_similarity(vect.transform([raw1]).toarray(),vect.transform([raw2]))\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:0\n",
      "199:1\n",
      "2019:2\n",
      "40:3\n",
      "990:4\n",
      "about:5\n",
      "abundance:6\n",
      "academic:7\n",
      "accomplish:8\n",
      "according:9\n",
      "accounts:10\n",
      "achingly:11\n",
      "acolyte:12\n",
      "across:13\n",
      "acting:14\n",
      "action:15\n",
      "activities:16\n",
      "activity:17\n",
      "actually:18\n",
      "added:19\n",
      "addition:20\n",
      "adds:21\n",
      "adequate:22\n",
      "adil:23\n",
      "admiration:24\n",
      "adulthood:25\n",
      "adventurer:26\n",
      "affair:27\n",
      "affairs:28\n",
      "affecting:29\n",
      "affection:30\n",
      "affections:31\n",
      "affinity:32\n",
      "after:33\n",
      "again:34\n",
      "against:35\n",
      "agriculture:36\n",
      "air:37\n",
      "algorithm:38\n",
      "all:39\n",
      "allowed:40\n",
      "allure:41\n",
      "ally:42\n",
      "alone:43\n",
      "alpine:44\n",
      "alps:45\n",
      "also:46\n",
      "always:47\n",
      "am:48\n",
      "amateur:49\n",
      "among:50\n",
      "an:51\n",
      "and:52\n",
      "anew:53\n",
      "animals:54\n",
      "another:55\n",
      "answer:56\n",
      "any:57\n",
      "anybody:58\n",
      "anything:59\n",
      "appreciative:60\n",
      "ardour:61\n",
      "are:62\n",
      "array:63\n",
      "art:64\n",
      "artist:65\n",
      "as:66\n",
      "aside:67\n",
      "assam:68\n",
      "assumed:69\n",
      "at:70\n",
      "attack:71\n",
      "attended:72\n",
      "attuned:73\n",
      "audio:74\n",
      "avowed:75\n",
      "azhar:76\n",
      "back:77\n",
      "backseat:78\n",
      "backwaters:79\n",
      "bangalore:80\n",
      "bare:81\n",
      "basic:82\n",
      "be:83\n",
      "beautiful:84\n",
      "beauty:85\n",
      "become:86\n",
      "been:87\n",
      "before:88\n",
      "beginnings:89\n",
      "being:90\n",
      "beliefs:91\n",
      "believed:92\n",
      "beloved:93\n",
      "bemusement:94\n",
      "better:95\n",
      "beyond:96\n",
      "bhat:97\n",
      "bhopal:98\n",
      "big:99\n",
      "birds:100\n",
      "blossoming:101\n",
      "bomber:102\n",
      "bones:103\n",
      "boogeyman:104\n",
      "book:105\n",
      "bookmarked:106\n",
      "books:107\n",
      "boosts:108\n",
      "boredom:109\n",
      "boulders:110\n",
      "bound:111\n",
      "break:112\n",
      "breaks:113\n",
      "broods:114\n",
      "budding:115\n",
      "burning:116\n",
      "but:117\n",
      "by:118\n",
      "can:119\n",
      "cape:120\n",
      "captivate:121\n",
      "car:122\n",
      "carried:123\n",
      "case:124\n",
      "casual:125\n",
      "catastrophic:126\n",
      "catch:127\n",
      "catnip:128\n",
      "caws:129\n",
      "centres:130\n",
      "challenged:131\n",
      "chance:132\n",
      "chandigarh:133\n",
      "change:134\n",
      "characterise:135\n",
      "charm:136\n",
      "charming:137\n",
      "chemistry:138\n",
      "chic:139\n",
      "chief:140\n",
      "child:141\n",
      "chilka:142\n",
      "chinks:143\n",
      "chirping:144\n",
      "chose:145\n",
      "christmas:146\n",
      "cities:147\n",
      "city:148\n",
      "clear:149\n",
      "clichã:150\n",
      "climate:151\n",
      "climbing:152\n",
      "clothes:153\n",
      "cochin:154\n",
      "coconut:155\n",
      "coimbatore:156\n",
      "cold:157\n",
      "collection:158\n",
      "come:159\n",
      "commissioner:160\n",
      "comorin:161\n",
      "compendium:162\n",
      "complex:163\n",
      "concerns:164\n",
      "confirms:165\n",
      "connection:166\n",
      "consonant:167\n",
      "context:168\n",
      "contrary:169\n",
      "control:170\n",
      "conversation:171\n",
      "conveyed:172\n",
      "convivial:173\n",
      "cooled:174\n",
      "copenhagen:175\n",
      "countries:176\n",
      "couple:177\n",
      "coupled:178\n",
      "course:179\n",
      "creed:180\n",
      "cross:181\n",
      "crow:182\n",
      "crpf:183\n",
      "culture:184\n",
      "cultures:185\n",
      "cut:186\n",
      "dal:187\n",
      "dar:188\n",
      "darjeeling:189\n",
      "daughter:190\n",
      "daunting:191\n",
      "dawns:192\n",
      "day:193\n",
      "days:194\n",
      "deal:195\n",
      "deaths:196\n",
      "december:197\n",
      "deep:198\n",
      "deepened:199\n",
      "definable:200\n",
      "delhi:201\n",
      "delhiite:202\n",
      "delightful:203\n",
      "demands:204\n",
      "descends:205\n",
      "desire:206\n",
      "despite:207\n",
      "details:208\n",
      "deterâ:209\n",
      "development:210\n",
      "devotees:211\n",
      "diarify:212\n",
      "did:213\n",
      "different:214\n",
      "differing:215\n",
      "dinner:216\n",
      "direct:217\n",
      "discovered:218\n",
      "dispels:219\n",
      "dispersing:220\n",
      "distance:221\n",
      "distant:222\n",
      "distraught:223\n",
      "diversity:224\n",
      "divorcee:225\n",
      "dizzying:226\n",
      "do:227\n",
      "doesn:228\n",
      "doesnt:229\n",
      "don:230\n",
      "dossier:231\n",
      "down:232\n",
      "dramatic:233\n",
      "dravidian:234\n",
      "dream:235\n",
      "drudgery:236\n",
      "during:237\n",
      "easily:238\n",
      "easy:239\n",
      "ebbs:240\n",
      "economy:241\n",
      "edi:242\n",
      "edition:243\n",
      "education:244\n",
      "educational:245\n",
      "educative:246\n",
      "effect:247\n",
      "electric:248\n",
      "else:249\n",
      "emanating:250\n",
      "emotion:251\n",
      "enchants:252\n",
      "endangering:253\n",
      "endearing:254\n",
      "endowed:255\n",
      "enduring:256\n",
      "enlighten:257\n",
      "enough:258\n",
      "escaped:259\n",
      "escapes:260\n",
      "especially:261\n",
      "essays:262\n",
      "establish:263\n",
      "etc:264\n",
      "europe:265\n",
      "even:266\n",
      "eventually:267\n",
      "every:268\n",
      "everything:269\n",
      "evidence:270\n",
      "evoking:271\n",
      "ex:272\n",
      "example:273\n",
      "exercise:274\n",
      "exhaustive:275\n",
      "existence:276\n",
      "expects:277\n",
      "experience:278\n",
      "extent:279\n",
      "external:280\n",
      "extra:281\n",
      "exuberant:282\n",
      "faces:283\n",
      "factories:284\n",
      "fail:285\n",
      "fairy:286\n",
      "falling:287\n",
      "famous:288\n",
      "fan:289\n",
      "far:290\n",
      "fat:291\n",
      "features:292\n",
      "february:293\n",
      "feeling:294\n",
      "festive:295\n",
      "fever:296\n",
      "feverish:297\n",
      "fidayeen:298\n",
      "fill:299\n",
      "filled:300\n",
      "find:301\n",
      "finding:302\n",
      "finer:303\n",
      "finland:304\n",
      "first:305\n",
      "flash:306\n",
      "flaunt:307\n",
      "flicker:308\n",
      "flows:309\n",
      "foolish:310\n",
      "for:311\n",
      "forget:312\n",
      "forms:313\n",
      "fort:314\n",
      "fray:315\n",
      "frequency:316\n",
      "fresh:317\n",
      "frida:318\n",
      "from:319\n",
      "frost:320\n",
      "full:321\n",
      "fun:322\n",
      "future:323\n",
      "gardens:324\n",
      "gather:325\n",
      "geographical:326\n",
      "geography:327\n",
      "german:328\n",
      "get:329\n",
      "gets:330\n",
      "getting:331\n",
      "ghazi:332\n",
      "given:333\n",
      "gives:334\n",
      "go:335\n",
      "goa:336\n",
      "going:337\n",
      "good:338\n",
      "gothenburg:339\n",
      "grapples:340\n",
      "grasp:341\n",
      "great:342\n",
      "greatest:343\n",
      "group:344\n",
      "groves:345\n",
      "gulmarg:346\n",
      "gusts:347\n",
      "had:348\n",
      "haider:349\n",
      "hallmark:350\n",
      "hammer:351\n",
      "handed:352\n",
      "handlers:353\n",
      "happy:354\n",
      "hardly:355\n",
      "has:356\n",
      "hatched:357\n",
      "haunts:358\n",
      "have:359\n",
      "he:360\n",
      "heal:361\n",
      "health:362\n",
      "hearts:363\n",
      "hello:364\n",
      "help:365\n",
      "helps:366\n",
      "her:367\n",
      "here:368\n",
      "hes:369\n",
      "high:370\n",
      "highly:371\n",
      "highroads:372\n",
      "hikes:373\n",
      "hiking:374\n",
      "hills:375\n",
      "him:376\n",
      "himalayas:377\n",
      "his:378\n",
      "hoarse:379\n",
      "home:380\n",
      "horizon:381\n",
      "hours:382\n",
      "how:383\n",
      "however:384\n",
      "howling:385\n",
      "huff:386\n",
      "human:387\n",
      "hypnotic:388\n",
      "idea:389\n",
      "ideas:390\n",
      "if:391\n",
      "ilk:392\n",
      "immediate:393\n",
      "immense:394\n",
      "importance:395\n",
      "important:396\n",
      "improve:397\n",
      "improves:398\n",
      "in:399\n",
      "inaccessible:400\n",
      "incentive:401\n",
      "india:402\n",
      "indian:403\n",
      "indication:404\n",
      "industrial:405\n",
      "industry:406\n",
      "information:407\n",
      "informative:408\n",
      "inhabit:409\n",
      "inner:410\n",
      "insists:411\n",
      "inspired:412\n",
      "instance:413\n",
      "intargile:414\n",
      "integral:415\n",
      "integration:416\n",
      "intensity:417\n",
      "internet:418\n",
      "intimate:419\n",
      "into:420\n",
      "involvement:421\n",
      "is:422\n",
      "it:423\n",
      "itinerary:424\n",
      "its:425\n",
      "itself:426\n",
      "iworldâ:427\n",
      "jaish:428\n",
      "jamshedpur:429\n",
      "jawans:430\n",
      "jem:431\n",
      "john:432\n",
      "joined:433\n",
      "journey:434\n",
      "journeys:435\n",
      "joyride:436\n",
      "just:437\n",
      "kaag:438\n",
      "kahlo:439\n",
      "kamran:440\n",
      "karl:441\n",
      "kashmir:442\n",
      "kharagpur:443\n",
      "killed:444\n",
      "knausgã:445\n",
      "know:446\n",
      "knowledge:447\n",
      "kolkata:448\n",
      "kovalam:449\n",
      "lake:450\n",
      "lakes:451\n",
      "land:452\n",
      "lands:453\n",
      "landscapes:454\n",
      "largest:455\n",
      "learn:456\n",
      "left:457\n",
      "letters:458\n",
      "lies:459\n",
      "life:460\n",
      "lifetime:461\n",
      "lights:462\n",
      "like:463\n",
      "limits:464\n",
      "listened:465\n",
      "living:466\n",
      "logtak:467\n",
      "long:468\n",
      "longevity:469\n",
      "look:470\n",
      "looked:471\n",
      "lost:472\n",
      "love:473\n",
      "lovers:474\n",
      "ma:475\n",
      "mad:476\n",
      "made:477\n",
      "madurai:478\n",
      "magazine:479\n",
      "make:480\n",
      "makes:481\n",
      "man:482\n",
      "manipur:483\n",
      "many:484\n",
      "marital:485\n",
      "marked:486\n",
      "marriages:487\n",
      "masood:488\n",
      "matters:489\n",
      "maulana:490\n",
      "may:491\n",
      "me:492\n",
      "meandering:493\n",
      "meditates:494\n",
      "mental:495\n",
      "mentioned:496\n",
      "mentions:497\n",
      "metecting:498\n",
      "mexico:499\n",
      "might:500\n",
      "miles:501\n",
      "mination:502\n",
      "minds:503\n",
      "ministry:504\n",
      "minus:505\n",
      "missives:506\n",
      "mode:507\n",
      "models:508\n",
      "modern:509\n",
      "mohammed:510\n",
      "moment:511\n",
      "monotony:512\n",
      "month:513\n",
      "more:514\n",
      "mosaic:515\n",
      "most:516\n",
      "mother:517\n",
      "mountaineer:518\n",
      "mountains:519\n",
      "move:520\n",
      "much:521\n",
      "mumbai:522\n",
      "must:523\n",
      "my:524\n",
      "nainital:525\n",
      "narrative:526\n",
      "national:527\n",
      "nature:528\n",
      "near:529\n",
      "negotiation:530\n",
      "never:531\n",
      "new:532\n",
      "ngti:533\n",
      "nietzsche:534\n",
      "no:535\n",
      "northern:536\n",
      "norwegian:537\n",
      "not:538\n",
      "novel:539\n",
      "novels:540\n",
      "now:541\n",
      "nuance:542\n",
      "numbing:543\n",
      "numerous:544\n",
      "obscure:545\n",
      "oceans:546\n",
      "october:547\n",
      "of:548\n",
      "offer:549\n",
      "offered:550\n",
      "often:551\n",
      "ogn:552\n",
      "old:553\n",
      "on:554\n",
      "once:555\n",
      "one:556\n",
      "oneâ:557\n",
      "only:558\n",
      "ooty:559\n",
      "open:560\n",
      "or:561\n",
      "organisation:562\n",
      "original:563\n",
      "orissa:564\n",
      "other:565\n",
      "otherâ:566\n",
      "our:567\n",
      "out:568\n",
      "outlines:569\n",
      "outlook:570\n",
      "outweigh:571\n",
      "ove:572\n",
      "over:573\n",
      "overcome:574\n",
      "overwhelming:575\n",
      "own:576\n",
      "owner:577\n",
      "pack:578\n",
      "page:579\n",
      "pakistan:580\n",
      "pakistani:581\n",
      "pan:582\n",
      "panchmari:583\n",
      "paris:584\n",
      "part:585\n",
      "parts:586\n",
      "passion:587\n",
      "past:588\n",
      "people:589\n",
      "peopleâ:590\n",
      "persists:591\n",
      "person:592\n",
      "philosopher:593\n",
      "philosophy:594\n",
      "phone:595\n",
      "physical:596\n",
      "physically:597\n",
      "picturesque:598\n",
      "pilgrimage:599\n",
      "place:600\n",
      "places:601\n",
      "plan:602\n",
      "plantations:603\n",
      "pleasant:604\n",
      "pleases:605\n",
      "pleats:606\n",
      "plenty:607\n",
      "point:608\n",
      "politics:609\n",
      "ponder:610\n",
      "pondicherry:611\n",
      "possible:612\n",
      "postcards:613\n",
      "poster:614\n",
      "potential:615\n",
      "practically:616\n",
      "praising:617\n",
      "precedes:618\n",
      "precise:619\n",
      "prefer:620\n",
      "prettiest:621\n",
      "principal:622\n",
      "pro:623\n",
      "problems:624\n",
      "process:625\n",
      "professional:626\n",
      "professor:627\n",
      "profound:628\n",
      "promoting:629\n",
      "protagonist:630\n",
      "protests:631\n",
      "proudly:632\n",
      "puff:633\n",
      "pulwama:634\n",
      "pure:635\n",
      "purposes:636\n",
      "put:637\n",
      "puâ:638\n",
      "questions:639\n",
      "quiet:640\n",
      "quirks:641\n",
      "quotes:642\n",
      "races:643\n",
      "radical:644\n",
      "ranchi:645\n",
      "rapturous:646\n",
      "rasheed:647\n",
      "rasping:648\n",
      "rated:649\n",
      "ravages:650\n",
      "rd:651\n",
      "re:652\n",
      "read:653\n",
      "realize:654\n",
      "recent:655\n",
      "recommendations:656\n",
      "recordings:657\n",
      "recruit:658\n",
      "reflect:659\n",
      "reindeer:660\n",
      "rekindle:661\n",
      "rekindles:662\n",
      "relationship:663\n",
      "relationships:664\n",
      "relax:665\n",
      "relaxation:666\n",
      "released:667\n",
      "remain:668\n",
      "remains:669\n",
      "remembrance:670\n",
      "reminded:671\n",
      "rendering:672\n",
      "reputation:673\n",
      "resos:674\n",
      "responsibilities:675\n",
      "revelations:676\n",
      "revisiting:677\n",
      "reâ:678\n",
      "rides:679\n",
      "risotto:680\n",
      "rivers:681\n",
      "road:682\n",
      "role:683\n",
      "rubber:684\n",
      "runway:685\n",
      "rush:686\n",
      "russia:687\n",
      "russian:688\n",
      "said:689\n",
      "sajjad:690\n",
      "salve:691\n",
      "same:692\n",
      "sartorially:693\n",
      "say:694\n",
      "scandinavian:695\n",
      "scene:696\n",
      "scholastic:697\n",
      "scoring:698\n",
      "screech:699\n",
      "se:700\n",
      "search:701\n",
      "searching:702\n",
      "seas:703\n",
      "season:704\n",
      "seduction:705\n",
      "seemingly:706\n",
      "self:707\n",
      "sentiment:708\n",
      "set:709\n",
      "shah:710\n",
      "shapes:711\n",
      "shillong:712\n",
      "short:713\n",
      "should:714\n",
      "showing:715\n",
      "siberia:716\n",
      "sick:717\n",
      "significant:718\n",
      "silences:719\n",
      "single:720\n",
      "skiing:721\n",
      "slipping:722\n",
      "slow:723\n",
      "so:724\n",
      "social:725\n",
      "soil:726\n",
      "solipsism:727\n",
      "solitary:728\n",
      "solitude:729\n",
      "solution:730\n",
      "some:731\n",
      "sometimes:732\n",
      "song:733\n",
      "souls:734\n",
      "sound:735\n",
      "sounds:736\n",
      "sources:737\n",
      "south:738\n",
      "spanning:739\n",
      "sparks:740\n",
      "special:741\n",
      "spent:742\n",
      "spice:743\n",
      "splendid:744\n",
      "splendour:745\n",
      "splã¼gen:746\n",
      "srinagar:747\n",
      "stands:748\n",
      "state:749\n",
      "states:750\n",
      "still:751\n",
      "stories:752\n",
      "story:753\n",
      "storytelling:754\n",
      "strong:755\n",
      "students:756\n",
      "style:757\n",
      "subdued:758\n",
      "subtly:759\n",
      "such:760\n",
      "suggested:761\n",
      "suicide:762\n",
      "summer:763\n",
      "summoned:764\n",
      "sunshine:765\n",
      "swedish:766\n",
      "switzerland:767\n",
      "syed:768\n",
      "take:769\n",
      "taken:770\n",
      "takes:771\n",
      "tales:772\n",
      "talking:773\n",
      "talks:774\n",
      "tapestry:775\n",
      "teateliers:776\n",
      "tee:777\n",
      "tempered:778\n",
      "tend:779\n",
      "tenet:780\n",
      "tenor:781\n",
      "terms:782\n",
      "territories:783\n",
      "terror:784\n",
      "terrorism:785\n",
      "terrorist:786\n",
      "test:787\n",
      "textbooks:788\n",
      "than:789\n",
      "that:790\n",
      "the:791\n",
      "their:792\n",
      "them:793\n",
      "then:794\n",
      "there:795\n",
      "therefore:796\n",
      "these:797\n",
      "they:798\n",
      "things:799\n",
      "think:800\n",
      "thinker:801\n",
      "this:802\n",
      "those:803\n",
      "thrill:804\n",
      "through:805\n",
      "thus:806\n",
      "time:807\n",
      "ting:808\n",
      "to:809\n",
      "told:810\n",
      "too:811\n",
      "took:812\n",
      "tortured:813\n",
      "touch:814\n",
      "tour:815\n",
      "tourism:816\n",
      "touriste:817\n",
      "toyotaaltis:818\n",
      "tracing:819\n",
      "traditions:820\n",
      "trails:821\n",
      "transcripts:822\n",
      "travel:823\n",
      "travellers:824\n",
      "travelling:825\n",
      "travelogue:826\n",
      "traversing:827\n",
      "tremendous:828\n",
      "tres:829\n",
      "trip:830\n",
      "trivial:831\n",
      "true:832\n",
      "trying:833\n",
      "turmoil:834\n",
      "turn:835\n",
      "ubiquity:836\n",
      "unborn:837\n",
      "under:838\n",
      "underrated:839\n",
      "understanding:840\n",
      "understood:841\n",
      "undertaking:842\n",
      "unforgettable:843\n",
      "unit:844\n",
      "unity:845\n",
      "unlike:846\n",
      "until:847\n",
      "up:848\n",
      "urgent:849\n",
      "us:850\n",
      "used:851\n",
      "valid:852\n",
      "valley:853\n",
      "valleys:854\n",
      "value:855\n",
      "values:856\n",
      "various:857\n",
      "verifiable:858\n",
      "video:859\n",
      "visions:860\n",
      "visit:861\n",
      "visiting:862\n",
      "visitor:863\n",
      "visits:864\n",
      "walk:865\n",
      "wanderlust:866\n",
      "wardrobe:867\n",
      "was:868\n",
      "way:869\n",
      "we:870\n",
      "wear:871\n",
      "weather:872\n",
      "were:873\n",
      "west:874\n",
      "what:875\n",
      "when:876\n",
      "where:877\n",
      "which:878\n",
      "while:879\n",
      "whittling:880\n",
      "who:881\n",
      "why:882\n",
      "widens:883\n",
      "wife:884\n",
      "will:885\n",
      "win:886\n",
      "winter:887\n",
      "with:888\n",
      "woman:889\n",
      "wonder:890\n",
      "woolly:891\n",
      "work:892\n",
      "world:893\n",
      "worship:894\n",
      "would:895\n",
      "wouldn:896\n",
      "writer:897\n",
      "writes:898\n",
      "writing:899\n",
      "writings:900\n",
      "www:901\n",
      "yangtected:902\n",
      "year:903\n",
      "you:904\n",
      "young:905\n",
      "your:906\n",
      "yuletide:907\n",
      "zeal:908\n"
     ]
    }
   ],
   "source": [
    "vocab=vect.vocabulary_\n",
    "for key in sorted(vocab.keys()):\n",
    "    print(\"{}:{}\".format(key, vocab[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.66018521]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect=CountVectorizer(binary=False)\n",
    "c=open('mixedcorpus.txt','r')\n",
    "corpus=[c.read()]\n",
    "vect.fit(corpus)\n",
    "f1=open('file2.txt','r')\n",
    "raw1=f1.read()\n",
    "raw1=raw1.lower()\n",
    "f1.close()\n",
    "f2=open('randomfile.txt','r')\n",
    "raw2=f2.read()\n",
    "raw2=raw2.lower()\n",
    "f2.close()\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similarity=cosine_similarity(vect.transform([raw1]).toarray(),vect.transform([raw2]))\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect=CountVectorizer(binary=False)\n",
    "c=open('mixedcorpus.txt','r')\n",
    "corpus=[c.read()]\n",
    "vect.fit(corpus)\n",
    "f1=open('blankfile.txt','r')\n",
    "raw1=f1.read()\n",
    "raw1=raw1.lower()\n",
    "f1.close()\n",
    "f2=open('randomfile.txt','r')\n",
    "raw2=f2.read()\n",
    "raw2=raw2.lower()\n",
    "f2.close()\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similarity=cosine_similarity(vect.transform([raw1]).toarray(),vect.transform([raw2]))\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
